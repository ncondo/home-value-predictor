{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "print(\"Python environment: {}\".format(sys.executable))\n",
    "\n",
    "import pandas as pd \n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "#from openpyxl import load_workbook\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "import plotly_express as px\n",
    "import matplotlib #collection of functions for scientific and publication-ready visualization\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "%matplotlib inline\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "import seaborn as sns # for making plots with seaborn\n",
    "color = sns.color_palette()\n",
    "print(\"seaborn version: {}\". format(sns.__version__))\n",
    "\n",
    "import numpy as np #foundational package for scientific computing\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "import scipy as sp #collection of functions for scientific computing and advance mathematics\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "print(\"SciPy version: {}\". format(sp.__version__)) \n",
    "\n",
    "import IPython\n",
    "from IPython import display #pretty printing of dataframes in Jupyter notebook\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "print(\"IPython version: {}\". format(IPython.__version__)) \n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from time import time\n",
    "\n",
    "# to make this notebook's output identical at every run\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "# sklearn modules for preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "# from imblearn.over_sampling import SMOTE  # SMOTE\n",
    "# sklearn modules for ML model selection\n",
    "from sklearn.model_selection import train_test_split  # import 'train_test_split'\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Libraries for data modelling\n",
    "from sklearn import svm, tree, linear_model, neighbors\n",
    "from sklearn import naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor # import RandomForestRegressor\n",
    "from sklearn.ensemble  import AdaBoostClassifier\n",
    "from sklearn.ensemble  import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Common sklearn Model Helpers\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "# from sklearn.datasets import make_classification\n",
    "\n",
    "# sklearn modules for performance metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve, recall_score, log_loss\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import r2_score, make_scorer, mean_squared_error\n",
    "print(\"scikit-learn libraries imported successfully\")\n",
    "\n",
    "# Other ML algorithms\n",
    "from lightgbm import LGBMRegressor\n",
    "print(\"lightgbm imported\")\n",
    "import xgboost as xgb\n",
    "print(\"xgboost imported\")\n",
    "# from mlxtend.regressor import StackingCVRegressor, StackingRegressor\n",
    "# print(\"StackingRegressor imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the supplied dataset and storing it in a dataframe\n",
    "training = pd.read_csv('train.csv')\n",
    "# making copies of original datasets for rest of this kernel\n",
    "df_train = training.copy()\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_train['SalePrice']  #target variable\n",
    "df_train = df_train.drop('SalePrice', axis=1) \n",
    "\n",
    "print(\"Training: {}, Target: {}, Test: {}\".format(df_train.shape, target.shape, df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_exp = df_train.copy() #make a copy of the training dataset for EDA purposes\n",
    "print(df_train_exp.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break down columns by data type\n",
    "print(\"{} Numerical columns, {} Categorial columns\".format(\n",
    "    list(df_train_exp.select_dtypes(include=[np.number]).shape)[1],\n",
    "    list(df_train_exp.select_dtypes(include = ['object']).shape)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_exp.columns.to_series().groupby(df_train_exp.dtypes).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of columns with missing values\n",
    "print(\"{} columns have missing values:\".format(\n",
    "    len(df_train_exp.columns[df_train_exp.isna().any()].tolist())))\n",
    "df_train_exp.columns[df_train_exp.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_exp.describe() # let's have a look at variable types in our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_exp.hist(figsize=(18,18))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for normal distribution hypothesis in numerical features\n",
    "test_normality = lambda x: stats.shapiro(x.fillna(0))[1] < 0.01\n",
    "numerical_features = [f for f in df_train_exp.columns if df_train_exp.dtypes[f] != 'object']\n",
    "normal = pd.DataFrame(df_train_exp[numerical_features])\n",
    "normal = normal.apply(test_normality)\n",
    "print(not normal.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations\n",
    "corr = training.corr(method='spearman')\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "# Heatmap\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(corr,\n",
    "            vmax=.5,\n",
    "            mask=mask,\n",
    "            #annot=True, \n",
    "            fmt='.2f',\n",
    "            linewidths=.2, cmap=\"YlGnBu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correlations with the target and sort\n",
    "correlations = training.corr(method='spearman')['SalePrice'].sort_values(ascending=False)\n",
    "correlations_abs = correlations.abs()\n",
    "print('\\nTop 10 correlations (absolute):\\n', correlations_abs.head(11))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Feature: SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_exp = target.copy() #make copy for exploratory purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see if there are any missing values (i.e. NA)\n",
    "print(\"There are {} NA values in 'SalePrice'\".format(target_exp.isnull().values.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = target_exp\n",
    "plt.figure(1); plt.title('Log Normal')\n",
    "sns.distplot(y, kde=False, fit=stats.lognorm)\n",
    "plt.ylabel('Frequency')\n",
    "print(\"Skewness: %f\" % target_exp.skew())\n",
    "# get mean and standard deviation\n",
    "(mu, sigma) = norm.fit(target_exp)\n",
    "print('Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get some stats on the 'SalePrice' variable\n",
    "print(\"Statistics for the supplied house prices training dataset:\\n\")\n",
    "print(\"Minimum price: ${:,.2f}\".format(np.min(target_exp)))\n",
    "print(\"Maximum price: ${:,.2f}\".format(np.max(target_exp)))\n",
    "print(\"Mean price: ${:,.2f}\".format(np.mean(target_exp)))\n",
    "print(\"Median price ${:,.2f}\".format(np.median(target_exp)))\n",
    "print(\"Standard deviation of prices: ${:,.2f}\".format(np.std(target_exp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  To get a visual of the outliers, let's plot a box plot.\n",
    "sns.boxplot(y = target)\n",
    "plt.ylabel('SalePrice (Log)')\n",
    "plt.title('Price');\n",
    "\n",
    "# count number of outliers after transformation is applied\n",
    "Q1 = target.quantile(0.25)\n",
    "Q3 = target.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(\"IQR value: {}\\n# of outliers: {}\".format(\n",
    "    IQR,\n",
    "    ((target < (Q1 - 1.5 * IQR)) | (target > (Q3 + 1.5 * IQR))).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transformation: Target Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying log transformation to the Target Variable\n",
    "target_tr = np.log1p(target)\n",
    "\n",
    "# let's plot a histogram with the fitted parameters used by the function\n",
    "sns.distplot(target_tr , fit=norm);\n",
    "(mu, sigma) = norm.fit(target_tr)\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.title('Price (Log)');\n",
    "print(\"Skewness: %f\" % target_tr.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  To get a visual of the outliers, let's plot a box plot.\n",
    "sns.boxplot(y = target_tr)\n",
    "plt.ylabel('SalePrice (Log)')\n",
    "plt.title('Price');\n",
    "\n",
    "# count number of outliers after transformation is applied\n",
    "Q1 = target_tr.quantile(0.25)\n",
    "Q3 = target_tr.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(\"IQR value: {}\\n# of outliers: {}\".format(\n",
    "    IQR,\n",
    "    ((target_tr < (Q1 - 1.5 * IQR)) | (target_tr > (Q3 + 1.5 * IQR))).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop redundant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['Id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = list(df_train.select_dtypes(\n",
    "        include=[np.number]).columns.values)\n",
    "categ_features = list(df_train.select_dtypes(\n",
    "    include=['object']).columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numeric_features:\n",
    "    df_train[col] = df_train[col].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_na = (df_train.isnull().sum()/len(df_train))*100\n",
    "ratio_na = perc_na.sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'Missing Values Ratio' :ratio_na})\n",
    "print(missing_data.shape)\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NaNs in categorical features with \"None\"\n",
    "df_train[categ_features] = df_train[categ_features].apply(lambda x: x.fillna(\"None\"), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing four numerical features with zero\n",
    "for col in (\"LotFrontage\", 'GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "    df_train[col].fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing other numerical features with median or mean\n",
    "impute_method = \"median\"\n",
    "\n",
    "if impute_method == \"median\": # replacing NaNs in numerical features with the median\n",
    "    df_train[numeric_features] = df_train[numeric_features].apply(\n",
    "        lambda x: x.fillna(x.median()), axis=0)\n",
    "    print(\"Missing values imputed with median.\")\n",
    "\n",
    "elif impute_method == \"mean\": # replacing NaNs in numerical features with the mean\n",
    "    df_train[numeric_features] = df_train[numeric_features].apply(\n",
    "        lambda x: x.fillna(x.mean()), axis=0)\n",
    "    print(\"Missing values imputed with mean.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"create combination of features.\")\n",
    "df_train['YrBltAndRemod']=df_train['YearBuilt']+df_train['YearRemodAdd']\n",
    "df_train['TotalSF']=df_train['TotalBsmtSF'] + df_train['1stFlrSF'] + df_train['2ndFlrSF']\n",
    "\n",
    "df_train['Total_sqr_footage'] = (df_train['BsmtFinSF1'] + df_train['BsmtFinSF2'] +\n",
    "                                 df_train['1stFlrSF'] + df_train['2ndFlrSF'])\n",
    "\n",
    "df_train['Total_Bathrooms'] = (df_train['FullBath'] + (0.5 * df_train['HalfBath']) +\n",
    "                               df_train['BsmtFullBath'] + (0.5 * df_train['BsmtHalfBath']))\n",
    "\n",
    "df_train['Total_porch_sf'] = (df_train['OpenPorchSF'] + df_train['3SsnPorch'] +\n",
    "                              df_train['EnclosedPorch'] + df_train['ScreenPorch'] + \n",
    "                             df_train['WoodDeckSF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"create boolean features.\")\n",
    "df_train['haspool'] = df_train['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "df_train['has2ndfloor'] = df_train['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "df_train['hasgarage'] = df_train['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "df_train['hasbsmt'] = df_train['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "df_train['hasfireplace'] = df_train['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformed numerical features that should be considered as strings.\")\n",
    "df_train['MSSubClass'] = df_train['MSSubClass'].apply(str)\n",
    "df_train['YrSold'] = df_train['YrSold'].astype(str)\n",
    "df_train['MoSold'] = df_train['MoSold'].astype(str)\n",
    "df_train['YrBltAndRemod'] = df_train['YrBltAndRemod'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = list(df_train.select_dtypes(include=[np.number]).columns.values)\n",
    "categ_features = list(df_train.select_dtypes(include=['object']).columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform numerical columns with skewness factor > 0.5\n",
    "# This is optional\n",
    "print(\"Transformed numerical columns with high skewness factor.\")\n",
    "skew_features = df_train[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "high_skew = skew_features[skew_features > 0.5]\n",
    "skew_index = high_skew.index\n",
    "for i in skew_index:\n",
    "    df_train[i] = boxcox1p(df_train[i], boxcox_normmax(df_train[i]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "df_train = pd.get_dummies(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features\n",
    "feature_scaling = \"RobustScaler\"\n",
    "\n",
    "if feature_scaling == 'MinMaxScaler':\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    for col in numeric_features:\n",
    "        df_train[[col]] = scaler.fit_transform(df_train[[col]])\n",
    "    print(\"Performed feature Scaling with MinMaxScaler.\")\n",
    "\n",
    "elif feature_scaling == 'StandardScaler':\n",
    "    scaler = StandardScaler()\n",
    "    for col in numeric_features:\n",
    "        df_train[[col]] = scaler.fit_transform(df_train[[col]])\n",
    "    print(\"Performed feature Scaling with StandardScaler.\")\n",
    "\n",
    "elif feature_scaling == \"RobustScaler\":\n",
    "    scaler = RobustScaler()\n",
    "    for col in numeric_features:\n",
    "        df_train[[col]] = scaler.fit_transform(df_train[[col]])\n",
    "    print(\"Performed feature Scaling with RobustScaler.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final df validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check that we no longer have any missing values\n",
    "perc_na = (df_train.isnull().sum()/len(df_train))*100\n",
    "ratio_na = perc_na.sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'missing_ratio' :ratio_na})\n",
    "missing_data = missing_data.drop(missing_data[missing_data.missing_ratio == 0].index)\n",
    "missing_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training and testing data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train,\n",
    "                                                    target_tr,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data Shape: {}\".format(df_train.shape))\n",
    "print(\"X_train Shape: {}\".format(X_train.shape))\n",
    "print(\"X_test Shape: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('Ridge Regression', Ridge(alpha=1.0)))\n",
    "models.append(('ElasticNet', ElasticNet()))\n",
    "models.append(('Random Forest', RandomForestRegressor(\n",
    "    n_estimators=100, random_state=7)))\n",
    "models.append(('Lasso', Lasso(random_state=42)))\n",
    "models.append(('XGBoost Regressor', xgb.XGBRegressor(objective='reg:squarederror', \n",
    "                                                     random_state=42)))\n",
    "models.append(('Gradient Boosting Regressor', GradientBoostingRegressor()))\n",
    "models.append(('LGBM Regressor',LGBMRegressor(objective='regression')))\n",
    "models.append(('SVR',SVR()))\n",
    "\n",
    "# set table to table to populate with performance results\n",
    "rmse_results = []\n",
    "names = []\n",
    "col = ['Algorithm', 'RMSE Mean', 'RMSE SD']\n",
    "df_results = pd.DataFrame(columns=col)\n",
    "\n",
    "# evaluate each model using cross-validation\n",
    "kfold = model_selection.KFold(n_splits=5, shuffle = True, random_state=7)\n",
    "i = 0\n",
    "for name, model in models:\n",
    "    # -mse scoring\n",
    "    cv_mse_results = model_selection.cross_val_score(\n",
    "        model, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
    "    # calculate and append rmse results\n",
    "    cv_rmse_results = np.sqrt(-cv_mse_results)\n",
    "    rmse_results.append(cv_rmse_results)\n",
    "    names.append(name)\n",
    "    df_results.loc[i] = [name,\n",
    "                         round(cv_rmse_results.mean(), 4),\n",
    "                         round(cv_rmse_results.std(), 4)]\n",
    "    i += 1\n",
    "df_results.sort_values(by=['RMSE Mean'], ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 8))\n",
    "fig.suptitle('Algorithm RMSE Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(rmse_results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning ML Hyper-Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor = xgb.XGBRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_xgb = {'n_estimators':range(10, 200, 10), \n",
    "             'learning_rate':[0.05,0.060,0.070], \n",
    "             'max_depth':[3,5,7],\n",
    "             'min_child_weight':[1,1.5,2]}\n",
    "grid_obj_xgb = RandomizedSearchCV(xgb_regressor, \n",
    "                                 parameters_xgb,\n",
    "                                 scoring = 'r2', \n",
    "                                 cv = 5,\n",
    "                                 n_jobs = -1,\n",
    "                                 n_iter = 100,\n",
    "                                 random_state= 99)\n",
    "grid_fit_xgb = grid_obj_xgb.fit(X_train, y_train)\n",
    "xgb_opt = grid_fit_xgb.best_estimator_\n",
    "\n",
    "print(\"best params: \" + str(grid_fit_xgb.best_params_))\n",
    "print('best score:', grid_fit_xgb.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2 on testing data\n",
    "r2_score(y_test, xgb_opt.predict(X_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "parameters = {'n_estimators':range(10, 200, 10), \n",
    "              'min_samples_leaf':range(5, 40, 5), \n",
    "              'max_depth':range(3, 5, 1)}\n",
    "grid_obj_rf = RandomizedSearchCV(rf_regressor, \n",
    "                                 parameters,\n",
    "                                 scoring = 'r2', \n",
    "                                 cv = 5,\n",
    "                                 n_jobs = -1,\n",
    "                                 n_iter = 100,\n",
    "                                 random_state= 99)\n",
    "grid_fit_rf = grid_obj_rf.fit(X_train, y_train)\n",
    "rf_opt = grid_fit_rf.best_estimator_\n",
    "\n",
    "print(\"best params: \" + str(grid_fit_rf.best_params_))\n",
    "print('best score:', grid_fit_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2 on testing data\n",
    "r2_score(y_test, rf_opt.predict(X_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_xgb = {'n_estimators': 190, 'min_child_weight': 2, 'max_depth': 3, 'learning_rate': 0.07}\n",
    "xgb_reg = xgb.XGBRegressor(**best_parameters_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb_reg.fit(df_train, target_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(xgb_model,  max_num_features=20 , importance_type='gain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_rf = {'n_estimators': 110, 'min_samples_leaf': 5, 'max_depth': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_regressor = RandomForestRegressor(**best_parameters_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rf_regressor.fit(df_train, target_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1] \n",
    "names = [df_train.columns[i] for i in indices] \n",
    "plt.figure(figsize=(15, 7)) \n",
    "plt.title(\"Top 10 Most Important Features\") \n",
    "plt.bar(range(10), importances[indices][:10]) \n",
    "plt.xticks(range(10), names[:10], rotation=90) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
